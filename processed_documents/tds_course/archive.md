---
source_url: "https://tds.s-anand.net/#/archive"
---

# Archived content

## Videos

### IITM BS Degree - Diploma Level Orientation

[**[Image Description]**: Here's a detailed description of the image:

1.  **What the Image Shows:** The image shows a screen capture of what appears to be a video conference or webinar session. Two men are visible on screen, presumably the presenters or participants. The background setting includes office-like elements. The interface indicates the session is conducted via "Webex" and mentions "IIT Madras - NPTEL / B".

2.  **Key Elements, Text, or Data Visible:**
    *   **Interface Text:** "Via Webex", "IIT Madras - NPTEL / B", "Andrew Thangaraj".
    *   **People:** Two men, both wearing glasses and shirts. One man has on a striped shirt and the other has on a plaid shirt.
    *   **Objects:** Books on a table, phones on the table.
    *   **Background:** A bookshelf with books, a door with blinds, a calendar on the wall, and dark upholstered chairs.

3.  **The Purpose or Educational Value:** The image suggests an educational or institutional context, likely a presentation or lecture related to IIT Madras and NPTEL (National Programme on Technology Enhanced Learning). The mention of "Webex" indicates it's an online learning or communication activity.

4.  **Specific Technical Details:**
    *   The image is formatted in a standard video conferencing layout with the presenter's video feed prominently displayed.
    *   The presence of the Webex logo and IIT Madras/NPTEL branding suggests the conference or session is official or affiliated with these institutions.
    *   The books on the table might hint at the topics discussed in the conference or related educational content.

*Original image: ![IITM BS Degree - Diploma Level Orientation, May 2022](https://img.youtube.com/vi_webp/Dj7X0bQRJSs/sddefault.webp)*](https://youtu.be/Dj7X0bQRJSs)

### Tools in Data Science Orientation

[**[Image Description]**: Here's a detailed description of the image:

1.  **What the image shows:**
    The image is a screenshot taken from a Zoom meeting. It appears to be a screen share as indicated by the Zoom logo visible in the lower right corner. The screen is black, with the text "Dixon" in the center.

2.  **Key elements, text, or data visible:**
    *   The name "Dixon" is displayed in the center of the screen.
    *   The "zoom" logo is in the bottom right corner.
    *   The overall dark appearance suggests a screen share from a user with no video feed enabled or a paused screen share.

3.  **The purpose or educational value:**
    The image lacks educational value on its own, beyond illustrating the appearance of a Zoom meeting screen share when a participant has not activated their video. Its purpose is likely to identify a participant named Dixon within the context of the orientation event described in the context.

4.  **Specific technical details:**
    *   The black screen indicates that no content is being shared on screen, either due to the user intentionally or due to some technical issue.
    *   The image format is likely a standard screenshot format, such as JPG or PNG.
    *   The presence of the Zoom logo is a key element, identifying the platform being used for communication.

*Original image: ![TDS Orientation, May 2022](https://img.youtube.com/vi_webp/_c_aFQ0ObLo/sddefault.webp)*](https://youtu.be/_c_aFQ0ObLo?t=4186)

### Tools in Data Science Live Sessions

[**[Image Description]**: Here is a detailed description of the image:

**General Overview**

The image is a screenshot of a screen showing multiple windows. It includes a web browser, a text editor/note-taking app, and a video conferencing window. The screenshot seems to be taken from an educational context, likely a lecture or presentation on data science.

**Detailed Description of Elements**

1.  **Web Browser Window**:
    *   Title on the tab: "Data sourcing- STM Dsc" (STM Dsc may refer to a specific course).
    *   A webpage related to 'Data Sourcing' is displayed.
    *   Rating: "5/5 (2 reviews)."
    *   Title: "TOOLS IN DATA SCIENCE," followed by "Get the Data Introduction."
    *   Instructor: "S Anand" (likely from IIT Madras).
    *   Text snippets: Mention of analyzing, visualizing, narrating, and deploying data.
    *   A URL link is visible: "https://youtube/LybbikB8cou."

2.  **Text Editor/Note-Taking App**:
    *   A plain text document named "Untitled" is opened.
    *   Content:
        *   "ROE" (possibly an acronym).
        *   "open exam: internet, GPT, LLM" (suggesting topics covered in the open exam).
        *   "syllabus: no, all, roughly week 2-5" (indicating the syllabus coverage timeframe).
        *   "Scraping" as a major topic, followed by details of how the lecturer would scrape for data.
        *   Categories for Scraping: HTML (live site like Wikipedia, local HTML files), Simple login, Authentication (Movie lens, Cookies) and XML files.

3.  **Video Conferencing Window**:
    *   A profile picture of a person, identified as "Carlton D'Silva," is visible. It suggests the presence of an instructor or participant in a video call.

**Purpose and Educational Value**

*   This image captures a learning session related to data science, particularly focusing on data sourcing.
*   The "Tools in Data Science" presentation title indicates the broader context of the learning material.
*   The notes in the text editor outline key concepts related to data scraping, serving as a syllabus or overview of the module.
*   The video conferencing window suggests interactive learning and the presence of an instructor guiding the session.

**Technical Details**

*   The text editor seems to be a built-in application in Windows.
*   The operating system appears to be Windows, likely Windows 10 or 11, based on the taskbar and interface.
*   "Activate Windows" watermark in the bottom-right indicates that the copy of Windows is not activated.
* The screenshot was taken on 14-07-2024 at 20:38.

**Summary**

The image shows an educational session covering data science topics, including data sourcing and web scraping. The session is likely being delivered online, involving a presentation, note-taking, and a video conference.

*Original image: ![TDS YouTube channel for live sessions](https://img.youtube.com/vi_webp/MQgOy5RNNz0/sddefault.webp)*](https://www.youtube.com/@se-lr5ff)

### Tools in Data Science Playlist

[**[Image Description]**: Here's a detailed description of the image:

1.  **Image Content**: The image appears to be a title slide for a video or presentation on data science. It features a person in the bottom right corner, likely the presenter, against a background of data-related graphics.

2.  **Key Elements and Text**:
    *   **Logo**: The "Gramener Insights as Stories" logo is visible at the top left.
    *   **Title**: The main title, "Discover the Data", is prominently displayed in white text.
    *   **Subtitle**: Below the title, "Tools in Data Science" is written in a smaller font.
    *   **Data Graphics**: The right side of the image is filled with hand-drawn-style graphics representing various types of data visualizations, including pie charts, bar graphs, line graphs, pyramids, checklists, and other icons associated with data analysis (light bulb with "IDEA", mail icon, checkmark, arrows, and percentages).
    *   **Presenter**: A person is shown in the bottom right corner, wearing headphones. The presenter seems to be an individual with short gray hair.

3.  **Purpose and Educational Value**:
    *   The image is designed to introduce or promote educational content related to data science, specifically focusing on the tools used in the field. The title "Discover the Data" suggests an introductory course or presentation.
    *   The graphics of different data visualization techniques hint at the topics that might be covered in the course or presentation.

4.  **Technical Details**:
    *   The background color is a deep blue or purple.
    *   The use of hand-drawn-style graphics gives the presentation a more accessible and less intimidating feel, which is useful for introductory material.

In summary, the image is the title slide for a data science presentation or course, emphasizing data discovery and tools. It uses a combination of text, logos, and data-related graphics to convey its message.

*Original image: ![Tools in Data Science Course Playlist , but only 17 videos](https://i.ytimg.com/vi_webp/3OeMOb7gByE/sddefault.webp)*](https://www.youtube.com/playlist?list=PLZ2ps__7DhBZEOxUBCkv61WHOu8m7ObpE)

[Tools in Data Science Course Playlist, 66 videos](https://youtube.com/playlist?list=PLZ2ps__7DhBZJ2q_hd8ZbDRgOJlB0CZLw&feature=shared)

## Optional: Parse & clean PDF files with Tabula

[**[Image Description]**: Here is a detailed description of the image:

1.  **What the image shows:** The image appears to be a presentation slide or a thumbnail for a video about data journalism. It features a background photo of a room full of people sitting at tables with laptops, seemingly engaged in a workshop or conference setting. Superimposed on this background are text and graphic elements.

2.  **Key elements, text, or data visible:**

*   The main text overlay reads "Getting Started With Data Journalism."
*   There are icons of a PDF file (with an Adobe logo) and a Microsoft Excel file, connected by an arrow pointing from the PDF to the Excel icon. The arrow indicates the conversion of a PDF file to an excel file.
*   The background image shows people working on laptops. Some screens appear to show spreadsheets or charts.
*   The text "Parse & clean PDF files with Tabula" is given as Alt text, suggesting the conversion is intended for data extraction and cleaning purposes.

3.  **The purpose or educational value:**

*   The image is likely designed to introduce or promote content related to data journalism. The title "Getting Started With Data Journalism" indicates that the content is aimed at beginners.
*   The PDF to Excel conversion graphic suggests that one key skill covered in the content will be how to extract data from PDF files and put it into a format (Excel) suitable for analysis. The phrase "Parse & clean PDF files with Tabula" indicates that this process may involve using a tool called Tabula to parse and clean data from PDF files.
*   The overall aim is to equip individuals with the basic skills to work with data in a journalistic context.

4.  **Specific technical details:**

*   The PDF icon signifies working with documents, which may contain tabular data or other structured information.
*   The Excel icon represents the destination format, suggesting that the end goal is to analyze the data in a spreadsheet program.
*   "Tabula" is an open-source tool for extracting tables from PDF files.
*   The process of extracting data from PDFs is often a preliminary step in data analysis, as PDF documents are designed for presentation rather than for easy data manipulation.

*Original image: ![Parse & clean PDF files with Tabula](https://i.ytimg.com/vi_webp/IEusn9HB1sc/sddefault.webp)*](https://youtu.be/IEusn9HB1sc)

- [Tabula documentation](https://tabula-py.readthedocs.io/en/latest/)

- Learn about the [`io` package](https://pymotw.com/3/io/), [reference](https://docs.python.org/3/library/io.html) and [video](https://youtu.be/cIaOisyd7lE)
- Learn about the [`os` package](https://pymotw.com/3/os/index.html), [reference](https://docs.python.org/3/library/os.html) and [video](https://youtu.be/tJxcKyFMTGo)

[**[Image Description]**: Here's a detailed description of the image:

1.  **Image Content:** The image appears to be a slide or title page from a presentation or lecture. It contains text and a logo.

2.  **Key Elements and Text:**
    *   The title is "Get the data: WIkimedia" (Note: There is a slight typographic error where the "i" in Wikimedia is lowercase).
    *   The course name is "TOOLS IN DATA SCIENCE".
    *   The instructor is "ANAND S".
    *   The tutorial instructor is "DIBU PHILIP".
    *   There's a logo in the top right corner, which is associated with "IIT Madras ONLINE DEGREE".
    * There is a horizontal line underneath "Get the data: WIkimedia"
    * There is a horizontal blue band across the bottom of the image.

3.  **Purpose and Educational Value:** The slide seems to introduce a topic related to data acquisition, specifically using Wikimedia as a data source. The slide likely represents a segment from a data science course focusing on tools and techniques.

4.  **Technical Details:**
    *   The image is likely a screen capture from a video lecture or online course.
    *   The slide is formatted with clear headings and instructor information.
    *  The slide suggests an activity of working with Wikimedia data.

*Original image: ![Wikipedia data with Wikimedia Python library](https://i.ytimg.com/vi_webp/b6puvm-QEY0/sddefault.webp)*](https://youtu.be/b6puvm-QEY0)

- Wikipedia Library - [Notebook](https://colab.research.google.com/drive/1UZky5JdOn2oMYIkls23WefTaT8VinYyg)
- Learn about the [`wikipedia` package](https://wikipedia.readthedocs.io/en/latest/)

## Image labelling with chess pieces

[**[Image Description]**: Here is a detailed description of the image:

1.  **Image Content:** The image shows a screenshot of a Windows file explorer window. The window displays a folder containing various images of chess pieces, specifically bishops. Additionally, there is a small video feed of a person visible in the bottom right corner.
2.  **Key Elements, Text, or Data Visible:**
    *   **File Explorer Window:** The file explorer shows the directory path, starting from "This PC" and navigating through "Desktop," "ITM_Online," "Image_Classification," "Input," "Chessman-image-dataset," "Chess," and finally "Bishop."
    *   **Image Files:** The folder is filled with numerous image files, each depicting a different bishop chess piece. The filenames follow a numerical pattern, starting from "00000000" and incrementing (e.g., "00000001", "00000002", etc.).
    *   **Folder Structure:** A navigation pane on the left side of the file explorer shows a folder structure that includes "Chessman" with subfolders like "Bishop," "King," "Knight," "Pawn," "Queen," and "Rook."
    *   **Text:** The title bar of the folder window reads "Bishop," indicating the folder's content. Other visible text includes "New," "Sort," "View," and various folder and file names.
    *   **Video Feed:** The video feed captures a man wearing a blue t-shirt, partially visible in the bottom right corner.
3.  **Purpose or Educational Value:** This image could be related to a project or tutorial on image classification, specifically focusing on identifying chess pieces. It likely illustrates the dataset organization for a machine learning model designed to recognize bishop chess pieces from images. The image could also be used for visual documentation or demonstration of image dataset preparation for AI/ML.
4.  **Specific Technical Details:** The image dataset seems to be organized into folders based on the type of chess piece. The naming convention for the image files suggests a structured way of organizing and accessing the data. This structure could be used to train an AI model. The number of images indicates the amount of data that might be used to train the model.

*Original image: ![Image labelling with chess pieces](https://i.ytimg.com/vi_webp/OTRamjRb7P4/sddefault.webp)*](https://youtu.be/OTRamjRb7P4)

- [Image dataset](https://www.kaggle.com/datasets/niteshfre/chessman-image-dataset)
- [Chess Pawns Excel file](https://docs.google.com/spreadsheets/d/156zEzw4al4Onx5IGPvhF8GbEj1TQ6Bqz/edit#gid=1998348575)
- [Jupyter Notebook](https://colab.research.google.com/drive/1xWILF9ifT2ifTYv4EXPUiB58Ts0-O9Bo?usp=sharing)
- Learn about the [PIL module](https://pypi.org/project/pillow/), [reference](https://pillow.readthedocs.io/en/stable/) and [video](https://youtu.be/dkp4wUhCwR4)
- Learn about the [BeautifulSoup module](https://beautiful-soup-4.readthedocs.io/en/latest/#quick-start) and [video](https://youtu.be/XVv6mJpFOb0)
- Learn about the [io module](https://pymotw.com/3/io/), [reference](https://docs.python.org/3/library/io.html) and [video](https://youtu.be/cIaOisyd7lE)

## Forecasting time series with Python

[**[Image Description]**: Here's a detailed description of the image:

**1. Image Content:**

The image is a screenshot of a Google Colaboratory notebook displaying code and visualization related to time series forecasting, specifically applied to COVID-19 data. A man is visible in the lower right corner, likely giving a lecture or tutorial on the topic.

**2. Key Elements, Text, and Data:**

*   **Google Colab Interface:** The standard interface of a Colab notebook is visible, including the menu bar (File, Edit, View, Insert, Runtime, Tools, Help), the code/text cells, and the runtime information.
*   **Notebook Title:** The notebook is titled "Covid19Forecasting.ipynb."
*   **Python Code:** Several code snippets are visible, including:
    *   Code for calculating moving averages:
        ```python
        data['moving_avg_5day'] = rolling_mean5
        data['moving_avg_10day'] = rolling_mean10
        data['moving_avg_15day'] = rolling_mean15
        data['moving_avg_20day'] = rolling_mean20
        data.head()
        ```
    *   Code for handling missing data and plotting:
        ```python
        data.dropna(inplace=True)
        autocorrelation_plot(data['new_cases'])
        plt.show()
        ```
    *   Import statement for statistical analysis
        ```python
        from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
        import statsmodels.api as sm
        ```
*   **Time Series Plot:** A plot showing the trend of COVID-19 cases over time is present. It shows the original time series data, along with moving averages calculated with different window sizes (5, 10, 15, and 20 days).  The x-axis shows dates from roughly February 2021 to July 2021. The y-axis likely represents the number of cases. Different colored lines depict the rolling mean for the number of days.

*   **Additional elements:**
    *   The name of the file "Covid19Forecasting.ipynb" is on the tab.
    *   In the top right corner, the RAM and Disk information for the session can be seen.

**3. Purpose and Educational Value:**

The image illustrates a practical application of Python and time series analysis techniques for forecasting disease spread (in this case, COVID-19). It demonstrates:

*   Data loading and preprocessing
*   Calculating rolling averages for smoothing time series data
*   Using autocorrelation plots to identify patterns in the data
*   Plotting and visualizing time series data and trends.
*   The notebook is likely part of a tutorial or project aimed at teaching these concepts.

**4. Technical Details:**

*   **Python Libraries:** The code uses popular Python data science libraries like `pandas` (implied by the use of `data[]`, `dropna`, and `head()`), `matplotlib` (implied by `plt.show()`), `statsmodels` for time series analysis.
*   **Time Series Concepts:**  The analysis utilizes the concept of moving averages, which are used to smooth out noise and highlight underlying trends in a time series. Autocorrelation plots help to identify the correlation of a time series with its lagged values, useful for understanding the time series' properties and choosing appropriate forecasting models.
*   **Colaboratory:** The choice of Google Colab suggests a focus on accessibility and ease of use, as Colab provides a free, cloud-based environment for running Python code.

*Original image: ![Forecasting time series with Python](https://i.ytimg.com/vi_webp/aedA2javxvE/sddefault.webp)*](https://youtu.be/aedA2javxvE)

- [Jupyter Notebook](https://colab.research.google.com/drive/1J62K0GG56ZNzq981AOTGwH_oA9w4RAYA?usp=sharing)
- [Understand time series modeling, moving averages and ARIMA](https://www.youtube.com/playlist?list=PLjwX9KFWtvNnOc4HtsvaDf1XYG3O5bv5s)
- Learn about the [pandas module](https://youtu.be/vmEHCJofslg) and [video](https://youtu.be/vmEHCJofslg)
- Learn about the [matplotlib module](https://matplotlib.org/stable/users/explain/quick_start.html) and [video](https://youtu.be/3Xc3CA655Y4)
- Learn about the [statsmodels module](https://www.statsmodels.org/stable/index.html) and [video](https://youtu.be/2BdfjqyWj3c)

## Data classification with Python

[**[Image Description]**: Here is a detailed description of the image:

1.  **Image Content:** The image is a screenshot of a Google Colaboratory notebook, which is used for interactive data analysis and machine learning. The notebook contains Python code along with visualizations and outputs. A person's face is visible in the bottom right corner, likely the presenter of the notebook.

2.  **Key Elements, Text, and Data:**
    *   **Notebook Title:** "Credit\_Card\_Decision\_Tree.ipynb" suggesting the project is about building a decision tree model for credit card decisions.
    *   **Python Code:** Several code snippets are visible:
        *   Seaborn (sns) scatterplots are being created, showing relationships between variables like 'CNT\_CHILDREN', 'AMT\_INCOME\_TOTAL', 'DAYS\_BIRTH', 'DAYS\_EMPLOYED', and 'CNT\_FAM\_MEMBERS'.
        *   The code includes arguments for the scatterplots, such as 'data=df' (referring to a DataFrame named 'df') and 'color="orange"' (setting the color of some points).
        *   There is code for removing outliers using the IQR (Interquartile Range) technique.
        *   The line `df.shape` shows the dimensions of the DataFrame `df` before processing, specifically (6471, 37).
    *   **Visualizations:** Several scatterplots are displayed as output from the Seaborn code. The plots show the relationship between different variables in the dataset.
    *   **Text in the Notebook:**
        *   "Cloud Storage for Work and ho..."
        *   "My Drive - Google Drive"
        *   "Tools for Data Science - Google"
        *   "Create\_Card\_Decision\_Tree.ipynb"
        *   "Removing Outliers using IQR Technique"
        *   Formulas for calculating IQR, lower range, and upper range are visible.
        *   Column names include 'AMT\_INCOME\_TOTAL' and 'CNT\_CHILDREN'.
    *   **IIT Madras Logo and Text:** A logo in the top right corner indicates that the notebook is associated with IIT Madras (Indian Institute of Technology Madras), and the text "BSc Degree" is displayed, which indicates a Bachelor of Science Degree.

3.  **Purpose and Educational Value:** The image likely captures a data science or machine learning tutorial or project demonstration. The purpose is to teach or illustrate:
    *   Data visualization using Seaborn.
    *   Data preprocessing techniques, specifically outlier removal using the IQR method.
    *   Building a decision tree model for credit card decision making (inferred from the notebook title).
    *   The process of exploring and cleaning data for machine learning.

4.  **Technical Details:**
    *   **Libraries:** Seaborn (sns) is being used for visualization.
    *   **Data Handling:** The code implies the use of Pandas (DataFrame `df`), though not explicitly mentioned in the visible code snippets.
    *   **IQR Method:** This is a statistical method for identifying and removing outliers.  The code shows the calculation of the IQR, lower and upper bounds, and subsequent filtering of the DataFrame.
    *   **Notebook Environment:** The code is being run in a Google Colaboratory environment, which is a free, cloud-based service for running Python code.
    *   **Python syntax:**  `q_hi = df['AMT_INCOME_TOTAL'].quantile(0.75)` refers to calculating the 75th percentile.
    *   **Data shape:** The initial dimensions of the DataFrame df are (6471 rows, 37 columns).

In summary, the image depicts a data science project focused on building a credit card decision model, showing data visualization, outlier removal, and is likely part of an educational resource from IIT Madras.

*Original image: ![Data classification with Python](https://i.ytimg.com/vi_webp/XsOihX38Bg0/sddefault.webp)*](https://youtu.be/XsOihX38Bg0)

- [Jupyter Notebook](https://colab.research.google.com/drive/1aW_sXdYK1UE9AA6TizQqlYTisFftn7T8?usp=sharing)
- [Understand machine learning](https://youtu.be/5q87K1WaoFI)
- [Understand decision trees](https://youtu.be/tNa99PG8hR8)
- Learn about the [pandas module](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) and [video](https://youtu.be/vmEHCJofslg)
- Learn about the [scikit-learn module](https://scikit-learn.org/stable/tutorial/basic/tutorial.html) and [video](https://youtu.be/pqNCD_5r0IU)
- Learn about the [matplotlib module](https://matplotlib.org/stable/users/explain/quick_start.html) and [video](https://youtu.be/3Xc3CA655Y4)

## Optional: R, RStudio and Rattle

R is a language and environment for statistical computing and graphics. It is open source and extremely popular for statistical analysis. In recent years, Python has become the de facto standard for data modeling. However, R still plays an important role for many analysts.

- [Getting started with R and RStudio](https://youtu.be/lVKMsaWju8w)
- [Learn about R](https://cran.r-project.org/manuals.html)
- [Learn about RStudio - an IDE for R](https://posit.co/products/open-source/rstudio/)
- [Learn about Rattle - a GUI for R](https://rattle.togaware.com)
- [Video: Rattle for Data Mining](https://youtu.be/OBilaZZpvGs)

## Automate machine learning with PyCaret

[**[Image Description]**: [Image description unavailable after multiple API key attempts.]

*Original image: ![Automate machine learning with PyCaret](https://i.ytimg.com/vi_webp/WMUt7NOJGbo/sddefault.webp)*](https://youtu.be/WMUt7NOJGbo)

- [Jupyter Notebook](https://colab.research.google.com/drive/1-gHL2lEEuKRFP40tkDMCNPron3F3p3iW?usp=sharing)

## Clustering with Python

[**[Image Description]**: Here's a detailed description of the image:

1.  **What the image shows:**
    *   The image shows a screenshot of a Google Colaboratory notebook, also known as a "Colab" notebook. Colab is a cloud-based service for running Python code, especially for data science and machine learning.
    *   The screenshot is focused on a data frame or table within the Colab notebook. This data frame appears to be the result of some kind of clustering analysis.

2.  **Key elements, text, or data visible:**
    *   **Title:** "Clustering.ipynb" suggests the notebook is about clustering algorithms or techniques.
    *   **Headers:** The data frame has several columns: "cluster", "size", "Price", "Market Cap", "Free Float Market Cap", "% to ADV", "RoI %", "ROCE %", "EBIT Margin %", "EPS", "PAT %", "Stock Return %".
    *   **Data:** The data consists of numerical values in each column, likely representing different financial metrics for each cluster. The data seems to be related to stock market analysis or investment.
    *   **Numerical Data:**
        *   Example values are "124", "1080.754", "2705.938", "44.352", "3.373", "34.230", "38.111", "22.061", "34.712", "17.210", "7.621" in the first row.
        *   There are six clusters (0 to 6).
    *   **Google Colab Interface:** The standard Google Colab menu bar and toolbar are visible, indicating that the code is being run in the Colab environment.
    *   **Code and Text Blocks:** There are options in the toolbar like "+ Code" and "+ Text", allowing adding code cells and text cells to the notebook, respectively.

3.  **The purpose or educational value:**
    *   The Colab notebook likely demonstrates how to perform clustering analysis on financial data using Python.
    *   It could be used to teach the following:
        *   Data analysis and manipulation with libraries like Pandas (which is likely used to create the data frame).
        *   Clustering algorithms (e.g., K-means, hierarchical clustering).
        *   Financial data interpretation.
        *   Using Google Colaboratory for data science projects.

4.  **Specific technical details:**
    *   The "cluster" column probably represents the cluster number each data point (likely a stock) belongs to.
    *   Other columns are likely financial metrics such as:
        *   **Price:** Stock price.
        *   **Market Cap:** Market capitalization.
        *   **Free Float Market Cap:** The portion of outstanding shares available for public trading.
        *   **% to ADV:** Percentage relative to average daily volume.
        *   **RoI%:** Return on Investment percentage.
        *   **ROCE%:** Return on Capital Employed percentage.
        *   **EBIT Margin %:** Earnings Before Interest and Taxes Margin percentage.
        *   **EPS:** Earnings per share.
        *   **PAT %:** Profit After Tax percentage.
        *   **Stock Return %:** Stock return percentage.
    *   The negative values in some of the columns (e.g., "RoI %", "EBIT Margin %", "Stock Return %") suggest losses or underperformance.

In summary, the image is a snapshot of a Colab notebook showcasing a financial data analysis project involving clustering techniques. It provides a glimpse into how data science can be used to group stocks based on various financial metrics.

*Original image: ![Clustering with Python](https://i.ytimg.com/vi_webp/lcMWH67TiWE/sddefault.webp)*](https://youtu.be/lcMWH67TiWE)

- [Jupyter Notebook](https://colab.research.google.com/drive/14-k1Pe0JgyLDsmzLeKPaWGCcQZktvMbg?usp=sharing)
- [How does K-Means clustering work?](https://youtu.be/4b5d3muPQmA)

## Sentiment analysis with Python and SpaCy

[**[Image Description]**: Here's a detailed description of the image provided:

**1. What the image shows:**

The image shows a screenshot of a Google Colaboratory (Colab) notebook focused on sentiment analysis. The Colab notebook is open in a web browser. The screenshot contains Python code, code output, and an inset video of a person.

**2. Key elements, text, or data visible:**

*   **Colab Notebook:** The title of the notebook is "ImdbTextClassification.ipynb". The notebook contains code cells, text cells, and output from executed code.
*   **Code Cells:**
    *   The code cells contain Python code using the TextBlob library to perform sentiment analysis on movie reviews.
    *   Specific lines of code involve applying lambda functions to calculate sentiment subjectivity and polarity.
    *   `data['TextBlob_Subjectivity'] = data['review'].apply(lambda x: TextBlob(x).sentiment.subjectivity)`
    *   `data['TextBlob_Polarity'] = data['review'].apply(lambda x: TextBlob(x).sentiment.polarity)`
    *   `data['TextBlob_Analysis'] = data['TextBlob_Polarity'].apply(lambda x: 'negative' if x<0 else 'positive')`
    *   `print(classification_report(data['sentiment'], data['TextBlob_Analysis']))`
*   **Text Cells:**
    *   The notebook contains text annotations like "I thought this was a wonderful way to spend ti... positive," which likely represents a review with its associated sentiment.
    *   "Basically there's a family where a little boy ... negative"
    *   "Petter Mattei's 'Love in the Time of Money' is... positive"
*   **Output:**
    *   The output of one of the executed cells is shown as: `data[['TextBlob_Subjectivity', 'Text']]`
*   **Other elements**
    *   Toolbar with options like "File", "Edit", "View", "Insert", "Runtime", "Tools", "Help."
    *   Resource monitor showing "RAM" and "Disk" usage.
    *   A thumbnail video of a person in the lower right corner.
    *   A progress bar and the text "2s completed at 8:38 PM" at the bottom, indicating the execution time.

**3. The purpose or educational value:**

The Colab notebook is designed for educational purposes to demonstrate how to perform sentiment analysis on text data using Python and the TextBlob library. It likely aims to teach:

*   Data preprocessing techniques.
*   Sentiment analysis concepts (subjectivity, polarity).
*   Using the TextBlob library.
*   Applying lambda functions for data transformation.
*   Evaluating the performance of a sentiment analysis model.

**4. Specific technical details:**

*   **Libraries:** The key library used is TextBlob, a Python library for processing textual data.
*   **Sentiment Analysis:** The code calculates sentiment polarity and subjectivity using TextBlob's built-in sentiment analysis capabilities. Polarity typically ranges from -1 (negative) to 1 (positive), and subjectivity ranges from 0 (objective) to 1 (subjective).
*   **Data Representation:** The data likely represents movie reviews (possibly from IMDb), and sentiment scores are being added as new columns to a Pandas DataFrame.
*   **Classification Report:** The code uses a `classification_report` function (likely from scikit-learn) to evaluate the performance of the sentiment analysis model by comparing the predicted sentiment with the actual sentiment.
*   **Lambda Functions:** Lambda functions are used to apply the TextBlob analysis to each review in the DataFrame.

*Original image: ![Sentiment analysis with Python and SpaCy](https://i.ytimg.com/vi_webp/A9WX7HaS1eU/sddefault.webp)*](https://youtu.be/A9WX7HaS1eU)

- [Jupyter Notebook](https://colab.research.google.com/drive/12SfxjYim6uijklYiByZCZDagTwPCF-MD?usp=sharing)
- Learn about the [pandas module](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html) and [video](https://youtu.be/vmEHCJofslg)
- Learn about the [TextBlob module](https://textblob.readthedocs.io/en/dev/) and [video](https://youtu.be/qTyj2R-wcks)

## Sentiment analysis with Excel and Azure ML

[**[Image Description]**: [Image description unavailable after multiple API key attempts.]

*Original image: ![Sentiment analysis with Excel and Azure ML](https://i.ytimg.com/vi_webp/wkbYLFEBCJg/sddefault.webp)*](https://youtu.be/wkbYLFEBCJg)

- [Excel Sentiment Analysis Workbook](https://docs.google.com/spreadsheets/d/1ZwGixdzUClEF9L1_Ec4t9zB8Ls3-zZdu/edit#gid=1600996918)
- [Excel Azure ML](https://appsource.microsoft.com/en-us/product/office/wa104379638?tab=overview)

## Image auto classification with Google Cloud Vision

[**[Image Description]**: Here's a detailed description of the image you provided:

**1. What the image shows:**

The image is a screenshot of the Google Cloud Platform (GCP) console, specifically the interface for Google Cloud Vision. It displays a project named "My First Project" and focuses on the "Images" section for a multi-label classification task. It shows the process of working with image datasets for machine learning purposes.

**2. Key elements, text, or data visible:**

*   **Google Cloud Platform Interface:** The top bar displays "Google Cloud Platform" with "My First Project" selected, and a search bar labeled "Search vision."
*   **Navigation:** The left sidebar shows options like "Dashboard," "Datasets," and "Models."
*   **Central Panel:**
    *   "untitled\_1634059901097" indicates the name of the dataset or project.
    *   Tabs for "IMPORT," "IMAGES," "TRAIN," "EVALUATE," and "TEST & USE" are visible, representing the stages of a machine learning workflow.
    *   The "IMAGES" tab is selected, and options are displayed: "All images" (550 images), "Labeled" (550), and "Unlabeled" (0).
    *   A "Filter" section allows you to filter images by label.
    *   Labels listed: "Bishop" (87), "King" (75), "Knight" (105), "Pawn" (104), "Queen" (77), and "ADD NEW LABEL."
    *   Image thumbnails are visible, including images of chess pieces. Labels such as "Rook(1)" and "Bishop(1)" appear below these thumbnails. One thumbnail includes the watermark "GOGRAPH."
    *   A pagination control indicates that the view shows "1-50 of" results, and the number of "Images per page" is set to "50."
*   **Chrome Tabs**: Multiple tabs are visible: Inbox, Cloud, Drive, Tools, Image etc.
*   **Bottom Right:** A person's webcam view is overlaid in the bottom right corner.

**3. The purpose or educational value:**

The screenshot likely demonstrates the use of Google Cloud Vision for image classification, specifically for training a model to recognize chess pieces. This is useful for:

*   **Educational purposes:** Explaining how to organize and label image datasets in Google Cloud Vision.
*   **Demonstrating Machine Learning workflow:** Showing the steps involved in training a model, from importing images and labeling them to training, evaluating, and testing the model.
*   **Illustrating Cloud Vision capabilities:** showcasing the features and interface of Google Cloud Vision for computer vision tasks.
*   **Potential Use Case:** Highlighting an example of image classification (in this case, chess piece recognition) that could be applied to other domains.

**4. Specific technical details:**

*   **Multi-Label Classification:** The use of the term "Multi-Label Classification" indicates that the model can potentially assign multiple labels to a single image.
*   **Dataset Size:** The number of images (550) gives an idea of the dataset size used for training.
*   **Label Imbalance:** The varying counts for each label (e.g., 87 Bishops, 75 Kings, 105 Knights) suggest a potential class imbalance that needs to be addressed during training.

In summary, this screenshot is a visual representation of setting up and managing an image classification project within the Google Cloud Vision platform, specifically focused on recognizing different chess pieces. It is likely part of a tutorial or demonstration aimed at teaching users how to use GCP for computer vision tasks.

*Original image: ![Image auto classification with Google Cloud Vision](https://i.ytimg.com/vi_webp/z4MUpn4FRTw/sddefault.webp)*](https://youtu.be/z4MUpn4FRTw)

- [Chessman image dataset](https://www.kaggle.com/datasets/niteshfre/chessman-image-dataset)
- [Google Cloud AutoML](https://cloud.google.com/automl/)

## Image classification with Python (Keras)

[**[Image Description]**: Here's a detailed description of the image:

**1. What the image shows:**

The image is a screenshot of a Google Colaboratory (Colab) notebook focused on image classification, specifically for chess pieces. It depicts a project structure and code related to training a machine learning model for this task.

**2. Key elements, text, or data visible:**

*   **Colab Interface:** The characteristic Colab interface is visible, including the file explorer on the left, the code area in the middle, and the Colab toolbar at the top.

*   **File Structure (File Explorer):**
    *   `Chessman-image-dataset`:  Indicates the main directory containing the chess piece images.
    *   Within `Chess`: Directories for each chess piece type are listed (Bishop, King, Knight, Pawn, Queen, Rook). This suggests a categorical image classification problem.
    *   `Test`, `Test2`: Possibly separate datasets for testing the model.
    *   `sample_data`: A default Colab directory.
    *   `chessman-image-dataset.zip`: The zipped dataset.
    *   `kaggle.json`: A file related to Kaggle, a platform for machine learning competitions, often used for authentication and data access.

*   **Image Examples:** The center of the screen shows sample images from the dataset: Bishop, Knight, and Pawn.

*   **Code Snippet:** A Python code snippet is visible. It is performing the following operations:
    *   `class_names = train.class_names`: retrieves the names of the classes from the 'train' dataset (most likely the name of each chess piece)
    *   `labels = np.array([])` : initialize an empty list to contain the labels
    *   `for _, label in train:`: loops through the image data in the training set
    *   `labels = np.concatenate((labels, np.argmax(label, axis=-1)))` this line extracts the class labels (chess piece type) from the training data by finding the index of the largest value (argmax) and concatenates them into the `labels` array.

*   **Performance Metrics:** "3s completed at 23:25" indicates that the code execution has completed and the running time was 3 seconds.

*   **Colab Resources:**  A RAM and Disk usage monitor is visible near the top-right, indicating available resources in the Colab environment.
*   **Person:**  A person is visible via a webcam in the bottom right.

**3. The purpose or educational value:**

The screenshot illustrates a typical image classification workflow in a Colab environment. It demonstrates how to:

*   Organize an image dataset by class.
*   Load and access data within a Colab notebook.
*   Write Python code for preparing data (label extraction using NumPy).
*   Monitor Colab resources.

This is valuable for anyone learning about image classification, machine learning, and the use of Colab for data science projects.

**4. Specific technical details:**

*   **Libraries:** The code snippet uses the NumPy library (`np`).
*   **Image Classification:** The task is multi-class image classification.
*   **Deep Learning Framework:** Although not explicitly stated, the use of `train.class_names` suggests that the project could involve deep learning framework like TensorFlow or PyTorch, which would be used for building and training the image classification model.
*   **Data Handling:** The code demonstrates a common data preparation step: converting categorical labels (names of chess pieces) into numerical labels suitable for machine learning algorithms. The `np.argmax` function is used to get the index of the class.

*Original image: ![Image classification with Python](https://i.ytimg.com/vi_webp/59u3XMiSyro/sddefault.webp)*](https://youtu.be/59u3XMiSyro)

- [Jupyter Notebook](https://colab.research.google.com/drive/1aU3eFkwRO-Ldu_QwmJ_JduRW7TOUVDlQ?usp=sharing)
- [Chessman image dataset](https://www.kaggle.com/datasets/niteshfre/chessman-image-dataset)
- Learn about the [keras module](https://keras.io) and [video](https://youtu.be/qFJeN9V1ZsI)
- Learn about the [pandas module](https://youtu.be/vmEHCJofslg) and [video](https://youtu.be/vmEHCJofslg)
- Learn about the [seaborn module](https://seaborn.pydata.org/tutorial.html) and [video](https://www.youtube.com/playlist?list=PL998lXKj66MpNd0_XkEXwzTGPxY2jYM2d)
- Learn about the [matplotlib module](https://matplotlib.org/stable/users/explain/quick_start.html) and [video](https://youtu.be/3Xc3CA655Y4)
- Learn about [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) and [video](https://youtu.be/LEPyAspkkew)

## Narratives with Quill on Tableau

-[**[Image Description]**: [Image description unavailable after multiple API key attempts.]

*Original image: ![Narratives with Quill on Tableau](https://i.ytimg.com/vi/b_Qojb9gLzA/sddefault.jpg)*](https://youtu.be/b_Qojb9gLzA)

## Comic narratives with Google Sheets & Comicgen

[**[Image Description]**: Here is a detailed description of the image:

1.  **What the image shows:**
The image is a screenshot of the Comicgen: Comic Creator webpage. It includes a drawn figure and various UI elements from the webpage. The image also includes a view of the presenter.

2.  **Key elements, text, or data visible:**
    *   **Text Content:** Several text elements are present, including:
        *   "Interested in data storytelling? Come join the #ComicgenFriday community."
        *   "Data Comicgen Awards"
        *   "Discuss on Twitter: #comicgen"
        *   "Suggest Improvements"
        *   "Copy link"
    *   **URL:** The URL field shows "gramener.com/comicgen/v1/#name=dey&text=Comic+speech+bubble+text&width=100&height=100&align=center&font-family=Architects+Daughte..."
    *   **Icons:** There are icons for SVG, PNG, a copy link, and a checkbox labeled "Background."
    *   **Comic character:** A simple cartoon character with hands folded.

3.  **The purpose or educational value:**
The image shows a web-based tool for creating comics, likely for data storytelling. It promotes the #ComicgenFriday community, suggesting that the tool is used or showcased on Fridays. The availability of SVG and PNG formats implies that users can download the generated comics. The "Data Comicgen Awards" button hints at a competition or recognition for comic creations made with the tool.

4.  **Specific technical details:**
The tool is likely developed by Gramener, as indicated by the text "Made by Gramener." The tool seems to allow customization via URL parameters. The URL in the address bar uses parameters like "name", "text", "width", "height", "align", and "font-family" which implies that the tool can generate dynamic content based on the defined parameters. The website licenses its code and images under the MIT License and CC-BY or CC0, respectively.

*Original image: ![Comic narratives with Google Sheets & Comicgen](https://i.ytimg.com/vi/HZDqCQBpHGI/sddefault.jpg)*](https://youtu.be/HZDqCQBpHGI)

- [Sample sheet](https://docs.google.com/spreadsheets/d/1b0DOfJnnx6MFcN955YqRqYafLb8XrH-zqtLaK2h5kkc/edit#gid=1534638946)

## Optional: Scaling hotstar.com for 25 million concurrent viewers

[**[Image Description]**: Here's a detailed description of the image:

**1. What the image shows:**

The image shows a frame from a video presentation. On the left, a man is giving a presentation.  On the right, a slide from the presentation is visible.

**2. Key elements, text, or data visible:**

*   **Presentation Slide:**
    *   **Brand:** "hotstar tech_" (The Hotstar logo is prominent with "tech_" written beneath it.)
    *   **Title:** "Scaling hotstar for 25 million concurrent viewers"
    *   **Speaker Information:** "Gaurav Kamboj", "Cloud Architect at Hotstar", "@oyehooye"
*   **Presenter:**
    *   The man on the left is wearing a maroon shirt and jeans.
    *   He has a microphone clipped to his shirt.
    *   He is gesturing with his hands as if explaining something.
    *   Part of a projected screen is visible behind him, showing some text snippets: "Scal ... hots ... 25 ... co" (presumably from a slide.)
*   An arm with a yellow shirt.

**3. The purpose or educational value:**

The image suggests the presentation's topic is how Hotstar, a streaming platform, handled scaling its infrastructure to support a large number of concurrent viewers (25 million). The presentation likely covers the technical strategies and challenges involved in accommodating such a large user base. It would be useful for anyone interested in cloud architecture, scalability, high-traffic systems, and the technology behind streaming services.

**4. Specific technical details:**

The title "Scaling hotstar for 25 million concurrent viewers" implies that the presentation will cover how Hotstar addressed the technical challenges of supporting such a large audience. This might involve discussions about:

*   Cloud infrastructure (given the speaker's title)
*   Content Delivery Networks (CDNs)
*   Load balancing
*   Database scaling
*   Network architecture
*   Optimization strategies to ensure a smooth viewing experience for all users.

The speaker's title "Cloud Architect at Hotstar" suggests the presentation will focus on the cloud-based solutions used to achieve this scalability.

*Original image: ![Optional : Scaling hotstar.com](https://i.ytimg.com/vi/QjvyiyH4rr0/sddefault.jpg)*](https://youtu.be/QjvyiyH4rr0)

## Supplementary material

- [Using Inspect element to find APIs](https://youtu.be/_gpBxglbDY4)
- Learn about the [urllib.parse package](https://docs.python.org/3/library/urllib.parse.html). [Video](https://youtu.be/LosIGgon_KM)
- Learn about the [os package](https://docs.python.org/3/library/os.html) and [video](https://youtu.be/tJxcKyFMTGo)

# Deployment

[**[Image Description]**: [Image description unavailable after multiple API key attempts.]

*Original image: ![Deployment](https://i.ytimg.com/vi_webp/YSGZjCxhIkk/sddefault.webp)*](https://youtu.be/YSGZjCxhIkk)

## Tools to anonymize data

[![Tools to anonymize data](https://i.ytimg.com/vi_webp/N8I-sxmMfqQ/sddefault.webp)](https://youtu.be/N8I-sxmMfqQ)

- [List of Tools](https://aircloak.com/top-5-free-data-anonymization-tools/)

## Libraries to build web applications

[**[Image Description]**: Here's a detailed description of the image:

1.  **What the image shows:** The image is a screenshot of a web application interface running in a web browser (Firefox). The application appears to be focused on credit card default prediction. There is a small inset showing a video feed of a person, likely the presenter or developer.

2.  **Key Elements, Text, and Data Visible:**
    *   **Title:** "Credit Card Default Prediction App"
    *   **Description:** "This app predicts the credit card Default probablity" (note the misspelling of "probability")
    *   **User Input Parameters:** This section has several input fields, presented as dropdown menus or selectable items:
        *   CODE\_GENDER (selected value: M)
        *   FLAG\_OWN\_CAR (selected value: Y)
        *   FLAG\_OWN\_REALTY (selected value: Y)
        *   FLAG\_WORK\_PHONE (selected value: D)
        *   FLAG\_EMAIL (selected value: D)
        *   FLAG\_PHONE (selected value: D)
    *   **URL Bar:** Shows the address `https://2385-36-00-66-102.ngrok.io` , which suggests that the application might be running on a local server using a tunneling service like ngrok.
    *   **Top Menu:** The Firefox browser menu is visible, with options like File, Edit, View, History, Bookmarks, Tools, Window, Help.
    *   **Taskbar:** The bottom of the screen displays the taskbar of a macOS system, with icons of applications like Finder, Safari, Mail, Calendar, App Store, etc.
    *   **Top Right Menu:** Displaying the system date: `Wed 9:30 AM`.

3.  **Purpose or Educational Value:** The application likely serves as a tool for predicting the likelihood of credit card default based on user-provided inputs. It could be used:
    *   **For Demonstration:** To showcase the functionality of a machine learning model for predicting credit risk.
    *   **For Educational Purposes:** To teach the concepts of credit risk modeling, feature engineering, or the use of web applications for data analysis.
    *   **As a Prototype:** As a basic version of a credit risk assessment tool.

4.  **Specific Technical Details:**
    *   The application is likely built using Python with a web framework such as Streamlit or Flask, given the context of building web applications.
    *   The use of ngrok indicates that the application is being hosted locally and exposed to the internet for testing or demonstration purposes.
    *   The input parameters suggest that the credit card default prediction model utilizes demographic and contact information as features.

In essence, the image presents a user interface for a credit card default prediction application, possibly built using Python web frameworks and hosted on a local server using ngrok. It is likely intended for demonstration, educational purposes, or as a prototype.

*Original image: ![Libraries to build web applications](https://i.ytimg.com/vi_webp/iT5sS1dWMcc/sddefault.webp)*](https://youtu.be/iT5sS1dWMcc)

- [List of frameworks](https://www.datarevenue.com/en-blog/data-dashboarding-streamlit-vs-dash-vs-shiny-vs-voila)
- [Code - Streamlit](https://github.com/rohithsrinivaas/streamlit-heroku)
- [Notebook - Streamlit](https://colab.research.google.com/drive/1Qd2xRdyd6SA8xaimUmlBDyu2FYnTpUar?usp=sharing)

## Services to host web applications

[**[Image Description]**: Here's a detailed description of the image you provided:

**1. What the Image Shows:**

The image is a screenshot of a web page, specifically from the Heroku platform, likely a user's dashboard or settings page. Heroku is a cloud platform as a service (PaaS) designed for deploying and managing web applications. The screenshot also shows a portion of the desktop environment, including a taskbar and a small video frame of a person.

**2. Key Elements, Text, or Data Visible:**

*   **Heroku Interface:**
    *   Title: "HEROKU"
    *   Tabs/Navigation: "Jump to Favorites, Apps, Pipelines, Spaces..."
    *   Section: "Add this app to a pipeline": Explains how to create or add an app to a new pipeline or existing one
    *   Section: "Add this app to a stage in a pipeline to enable additional features": Explains how pipelines let you connect multiple apps together and promote code between them.
    *   Section: "Pipelines connected to Github can enable review apps, and create apps for new pull requests."
    *   Section: "Deployment Method": Heroku Git (Use Heroku CLI) and Github (Connected)
    *   Section: "App connected to GitHub": Indicates a connection to a GitHub repository, mentioning commit diffs and manual and auto deploys
    *   Section: "Connected to <repository name> by <user name>": Shows a specific GitHub repository and the user who connected it. There is also a "Disconnect" button next to this line.
    *   Section: "Automatic Deploys": Enables a chosen branch to be automatically deployed to this app.
    *   Section: "You can now change your main deploy branch from master to main for both manual and automatic deploys, please follow the instructions here."
    *   Section: "Enable automatic deploys from Github". Deploys automatically be sure that this branch is always in a deployable state.

*   **Browser Elements:**
    *   Chrome browser interface (address bar, tabs, icons).
    *   Tabs such as "Tools for Data Science - Goog...", "credit-decline - GitHub / Herok...", "streamit in heroku - Google S...", "A quick tutorial on how to doc..."

*   **Desktop Environment:**
    *   Apple dock at the bottom.
    *   A video frame (presumably a user's webcam feed).

**3. Purpose or Educational Value:**

*   The image is likely part of a tutorial or documentation explaining how to connect a Heroku application to a GitHub repository and set up a deployment pipeline.
*   It demonstrates the Heroku interface for configuring automated deployments and integrating with GitHub.
*   It could be used to teach developers how to automate the process of deploying updates to their web applications from a GitHub repository.

**4. Specific Technical Details:**

*   **GitHub Integration:** The image highlights the integration between Heroku and GitHub, enabling automated deployments and continuous integration workflows.
*   **Deployment Pipelines:** The screenshot showcases Heroku's pipeline feature, which allows for managing different stages of an application's lifecycle (e.g., development, staging, production).
*   **Automatic Deploys:** The image focuses on configuring automatic deployments, where changes committed to a specific branch in the GitHub repository automatically trigger a deployment to Heroku.
*   **Branch Selection:** The user can choose a specific branch in their GitHub repository to be automatically deployed.

In summary, the image provides a visual guide to setting up automatic deployments from GitHub to Heroku, useful for developers looking to streamline their deployment process.

*Original image: ![Services to host web applications](https://i.ytimg.com/vi_webp/V5dl7zkKXC0/sddefault.webp)*](https://youtu.be/V5dl7zkKXC0)

- [List and comparison](https://sourceforge.net/software/compare/Glitch-vs-Heroku-vs-Netlify-vs-Vercel/)
- [Heroku deployment](https://www.heroku.com/home)
- Docker/Podman
- GitHub actions
- Glitch.me

# Data Discovery

[**[Image Description]**: Here's a detailed description of the image:

1.  **Visual Description:**

    *   The image appears to be a title slide from a data science presentation or video. The dominant color is a deep blue.
    *   On the right side, there's a cluster of white line drawings representing various data-related concepts and visualisations. These include bar charts, pie charts, line graphs, pyramids, light bulb symbols labeled "IDEA," arrows, envelopes, checklists, and symbols of money. It looks like a hand-drawn infographic.
    *   In the bottom right corner, there's a small video feed of a man wearing headphones, likely the presenter.

2.  **Key Elements and Text:**

    *   The logo "Gramener, Insights as Stories" is prominently displayed in the upper left corner.
    *   The title "Discover the Data" is centered and written in a large, white font.
    *   Below the title, in a smaller font and a lighter shade of orange, the text "Tools in Data Science" is visible.
    *   A single capital letter "G" is present in the lower left corner.

3.  **Purpose and Educational Value:**

    *   The image serves as a title or introductory slide for a presentation, webinar, or video about data science, specifically focusing on data discovery.
    *   The visuals suggest the presentation will likely cover a range of data analysis techniques and tools.

4.  **Technical Details:**

    *   The graphics style is clean and modern.
    *   The font used for the title is bold and easily readable.
    *   The use of hand-drawn elements might indicate a creative or storytelling approach to data presentation.

*Original image: ![Data discovery](https://i.ytimg.com/vi_webp/3OeMOb7gByE/sddefault.webp)*](https://youtu.be/3OeMOb7gByE)

Before we begin the data science journey, you first need the data set. And to get the
data set, you need to know where it is. This is what we will be covering in the first module.

How do you discover data? There are three things that you will learn in this module.

- The first is, what are the different sources of data sets? Where can you find them?
- The second is, what are the different kinds of data sets? Structured, unstructured and semi-structured.
- Third, in each data set, what are the different kinds of values that you will find?

This will give you a sense of locating the kind of data set that you want, either on the internet
or within your organization or even within your phones.

## Sources of Data

[**[Image Description]**: Here is a detailed description of the image:

1.  **Image Content:** The image is a screenshot from a presentation or educational video, featuring a slide titled "Sources of datasets." It's designed to categorize and highlight different types of data sources. The layout consists of three orange rectangular boxes labeled "Public Data," "Private Data," and "Personal Data." Underneath the "Public Data" box is a list of specific public data resources. Additionally, there is a video feed of a presenter in the lower right corner.

2.  **Key Elements and Text:**
    *   **Title:** "Sources of datasets" at the top of the slide.
    *   **Category Boxes:**
        *   "Public Data"
            *   "Awesome Public Datasets"
            *   "Google Dataset Search"
            *   "Kaggle"
            *   "Data.gov.*"
        *   "Private Data"
        *   "Personal Data"
    *   **Presenter:** A middle aged man wearing headphones and a dark shirt in a video feed at the bottom right corner.
    *   **Logo:** A simple "G" logo in the bottom left corner.

3.  **Purpose and Educational Value:** The slide aims to educate viewers about the different categories of data sources available. It differentiates between public, private, and personal data. By listing specific examples under "Public Data," it provides concrete resources for finding publicly available datasets, which is helpful for data science, research, and education.

4.  **Specific Technical Details:**
    *   The slide utilizes a dark blue background with orange labels for the categories.
    *   The font appears to be clear and readable.
    *   The presence of a presenter suggests that the slide is part of a more extensive presentation or video tutorial.
    *   The use of bullet points underneath "Public Data" helps to list the options in a concise manner.

*Original image: ![Sources of Data](https://i.ytimg.com/vi_webp/GY5l_5RpVZM/sddefault.webp)*](https://youtu.be/GY5l_5RpVZM)

- [Awesome public datasets](https://github.com/awesomedata/awesome-public-datasets)
- [Google dataset search](https://datasetsearch.research.google.com/)
- [Kaggle datasets](https://www.kaggle.com/datasets/)
- [Data.gov](https://data.gov/) and [Data.gov.in](https://data.gov.in/)
- [Datameet](https://datameet.org/)

## Types of datasets

[**[Image Description]**: Here's a detailed description of the image:

1.  **What the image shows:** The image is a screenshot of a presentation slide, likely from a video lecture or webinar. It primarily features a visual representation of different types of datasets and has a small inset video of a person in the bottom right corner.

2.  **Key elements, text, or data visible:**
    *   **Title:** The slide's title is "Types of datasets," indicating the presentation's subject matter.
    *   **Categorization of Datasets:** The slide displays three categories of datasets:
        *   **Structured:** This category has "Structured" printed in orange on the top of a block, and below it, a subcategory "Databases" written in black.
        *   **Semi-structured:** This category simply has "Semi-structured" written in orange.
        *   **Unstructured:** This category simply has "Unstructured" written in orange.
    *   **Person in Video Inset:** A small video inset at the bottom right shows a man with gray hair wearing headphones, likely the presenter.

3.  **The purpose or educational value:** The primary purpose of the image is educational. It aims to visually communicate the basic categories of datasets (structured, semi-structured, and unstructured) to an audience. It also gives an example of a structured database (Databases). This would typically be part of a broader lesson on data science, data management, or related topics.

4.  **Specific technical details:**
    * The background color is a dark blue shade.
    * The text and blocks indicating the types of datasets are arranged horizontally across the screen.
    * The use of different colors (orange, white, and blue) is likely for emphasis and visual clarity.
    * The use of a video inset suggests this slide is part of a dynamic presentation or lecture.

*Original image: ![Types of datasets](https://i.ytimg.com/vi_webp/u8PIxqsi1kk/sddefault.webp)*](https://youtu.be/u8PIxqsi1kk)

- Structured data has a schema: Databases, Spreadsheets, Forms, Shapefiles
- Semi-structured data has a flexible schema: JSON, HTML, Email
- Unstructured data has no schema: Text, Images, Audio, Video
- [DBF opener](https://www.dbfopener.com/)
- [MapShaper lets you view Shapefiles](https://mapshaper.org/)

## Types of values

[**[Image Description]**: Here's a detailed description of the image:

1.  **What the image shows:** The image is a screenshot of a presentation slide. It categorizes and illustrates "Types of values," a concept likely related to data types or data analysis. The slide is primarily composed of text and rectangular boxes. There is also a small video in the bottom right of a man speaking, likely the presenter.

2.  **Key elements, text, or data visible:**
    *   **Title:** "Types of values" at the top center of the screen.
    *   **Categorical Types:** Three orange rectangles are labeled as:
        *   "Categorical" with a list of sub-categories and examples below:
            *   "Boolean (True, False)"
            *   "Unordered (Red, Blue, Green)"
            *   "Ordered (Low, Medium, High)"
        *   "Numerical"
        *   "Composite"
    *   The letter "G" at the bottom left corner of the screen.

3.  **The purpose or educational value:** The slide appears to be part of a lesson or tutorial on data types or variable types. It aims to clarify the different categories of values that can be encountered, especially in the context of data analysis or programming. The examples provided help to distinguish between different sub-types of categorical data (boolean, unordered, ordered).

4.  **Specific technical details:** The screenshot displays a conceptual overview rather than specific technical code or equations. The categories suggest a focus on different ways data can be represented and handled in data processing or software development. The terms "Categorical," "Numerical," and "Composite" point to a classification of value types based on their nature and characteristics. The choice of examples (True/False for Boolean, colors for Unordered, and ordinal values for Ordered) highlights the nuances within the Categorical type.

*Original image: ![Types of values](https://i.ytimg.com/vi_webp/HlsqT0r9wAM/sddefault.webp)*](https://youtu.be/HlsqT0r9wAM)

- Categorical values may be:
  - Boolean: True or False
  - Unordered: No order, like colors
  - Ordered: Order, like ratings
  - Cyclical: Like days of the week
  - Unstructured: Like names, images
- Numerical values may be:
  - Integer: You can add or subtract
  - Real: You can multiply or divide
- Composite values have an internal structure
  - Temporal: Date, Time
  - Spatial: Latitude, Longitude, Shapefiles
  - Structured: JSON, XML with schema
  - Specialized: IP addresses, URLs, Email addresses, Phone numbers, etc.

## Week Summary

[**[Image Description]**: Here is a detailed description of the image:

1.  **Visual Content:** The image is a screen capture of what appears to be a presentation or video conference about data science. The left side of the screen displays the title "Discover the Data" followed by "Tools in Data Science". The backdrop behind the titles features white line drawings representing various types of data visualizations, such as bar charts, pie charts, line graphs, and infographic icons like lightbulbs labeled "IDEA" and percentage displays. On the lower right side, there's a video feed of a man in a headset speaking or presenting.

2.  **Key Elements and Text:**
    *   **Gramener:** The top-left corner shows the logo of "Gramener" with the tagline "Insights as Stories."
    *   **Titles:** "Discover the Data" is the main title in large font, with "Tools in Data Science" below in a smaller font.
    *   **Data Visualization Icons:** The background contains illustrations of various charts and graphs associated with data analysis.
    *   **Video Feed:** The man in the video appears to be the presenter or speaker.
    *   **Percentage Displays:** Pie charts with "100%" labeled on them, indicating complete proportions.

3.  **Purpose and Educational Value:** The image seems to be part of a data science presentation or educational video focused on discovering or exploring data and tools for data science. The presentation aims to provide insights through data storytelling, possibly showcasing different tools and methods for data analysis and visualization.

4.  **Technical Details:**
    *   **Layout:** The layout is a slide-like presentation with a title and a live video feed, suggesting a webinar or online lecture.
    *   **Data Visualizations:** The use of data visualization icons implies the presentation's focus on visually representing data insights.

In summary, the image captures a moment from a data science presentation, likely an introduction to tools and methods used in the field, with a visual theme of data visualization elements.

*Original image: ![Discover the Data - Summary](https://i.ytimg.com/vi_webp/NNiFxgANu8Y/sddefault.webp)*](https://youtu.be/NNiFxgANu8Y)

Based on what you have learnt in this module, you should be able to do two things: find data and understand what type of data it is.

Both of these are powerful skills.

The more data you are able to find, the more analysis that you will be able to do that others are unable to.
Therefore, discovering new sources of data is a competitive advantage and a skill that is well worth building.

The other, in terms of understanding the type of data, will give you an edge in terms of knowing which data set is easier to work with.
Structured data is easier to work with because you don't have to do any additional work.
You don't have to extract information from it.
Numerical values are easier to work with than, let's say, categorical or composite because there's less effot to extract the structure.
So you'll be able to compare two data sets and say that one gets more results by spending in less time and effort.

## Sample questions

- Find the UCI machine learning dataset on Wine Quality. (It has 4,898 rows.) What is the highest pH value of the red wines? (ANS: 4.01)
- What's the official data portal of Russia? (ANS: <https://data.gov.ru/?language=en>)
- Are research papers structured, semi-structured or unstructured? (ANS: Semi-structured. They have author names, abstracts, keywords, etc. but most content is free-form.)
- Are book titles categorical or composite? (ANS: Categorical. They don't have an underlying structure.)

## Optional: Tools used in the industry

Kathir Mani from QueLit and Anand S from Gramener discuss the tools and technologies used in the industry. (9 min)

[**[Image Description]**: Here's a detailed description of the image:

1.  **What the image shows:**
    *   The image is a split-screen shot, likely from a video call or podcast recording. It features two individuals displayed side-by-side.

2.  **Key elements, text, or data visible:**
    *   **Left Panel:** Shows a man with light skin, short grey hair, and a dark shirt. A white bookshelf is visible behind him.
    *   **Right Panel:** A man with darker skin, short black hair, and a blue striped shirt is present. The background reveals what seems to be a restaurant or cafe setting with a wall decorated with text and a neon octopus. The text "bour" is partially visible on the wall.
    *   **General:** Both men are facing the camera and appear to be engaged in a conversation.

3.  **Purpose or educational value:**
    *   This image most likely serves as a visual representation of a conversation or podcast episode, featuring the two individuals. It could be used to promote the podcast or video content. The visual element aims to engage viewers and provide a face to the voices or content.

4.  **Specific technical details:**
    *   The split-screen layout indicates a recording setting (e.g., video call, podcast), where individuals are remotely connected.
    *   The backgrounds of each person offer insights into their locations or settings. The man on the left is in a home or office setting, while the man on the right seems to be in a public space like a restaurant.

*Original image: ![Podcast (9 min)](https://i.ytimg.com/vi_webp/DH0Q4LiSgkE/sddefault.webp)*](https://youtu.be/DH0Q4LiSgkE)

## Scraping: Reference and helpful content

- For those who don't know HTML, CSS, or JavaScript, this [FreeCodeCamp Responsive Web Design course](https://www.freecodecamp.org/learn/2022/responsive-web-design) is a good starting point.
- For those who don't know Python, this [Learn Python video](https://youtu.be/rfscVS0vtbw) and this [Python for Beginners](https://youtube.com/playlist?list=PLsyeobzWxl7poL9JTVyndKe62ieoN-MZ3&feature=shared) playlist is a good starting point.
- Few More Scraping tools
  1. [About Scrapy & Chrome Web Scraper Extension](https://docs.google.com/document/d/1QZPJIfg98-Gox7_tqzrqPy9PigYfRfgpsYgTHcBAYFM/view) [(Video)](https://youtu.be/s4jtkzHhLzY)
  2. Chrome Web Scraper Extension [(Video)](https://youtu.be/aClnnoQK9G0)

## Scraping: Sample questions

- Read the [Hacker News API docs](https://github.com/HackerNews/API). Now, when was the post with ID `2921983` posted? Specifically, What is the timestamp? (ANS: 1314211127)
- Using [PokeAPI](https://pokeapi.co/), in the `sun-moon` version, find out how many moves `ivysaur` has that `bulbasaur` does not. (ANS: 1: leech-seed).
- How many images (`<img>` tags) does this [White House page snapshot](https://web.archive.org/web/20110101070603id_/https://www.whitehouse.gov/) have inside a link (`<a>` element)? (ANS: 15)
- What is the westernmost point (highest longitude) on the bounding box of `Baghdad, Iraq`, according to the Nominatim API? If there are multiple matches, get the highest longitude across all bounding boxes. (ANS: 44.969E)
- What CSS selector would you use to extract the last list element with a class `highlight` from an unordered list? (ANS: `ul li.highlight:last-child`)

## Smart Narratives with Power BI

[**[Image Description]**: Here's a detailed description of the image:

**1. What the image shows:**

The image displays a screenshot of the Power BI desktop application. It primarily showcases a bar chart and several panels within Power BI's user interface.  A person is visible in the lower-right corner, presumably presenting or interacting with Power BI.

**2. Key elements, text, or data visible:**

*   **Bar Chart:**  The central element is a blue bar chart titled "UN 2018 population estimates(s) by City(s)". The chart displays the population estimates for various cities, with city names along the horizontal axis (x-axis) and population (likely in millions) on the vertical axis (y-axis). A context menu appears when right-clicking on a data point, displaying "Show as a table, Analyze, Share, Summarize, Copy".
*   **Power BI Panels:** On the right side of the interface, there are Power BI panels, including:
    *   **Filters:** This panel allows the user to filter the data displayed in the chart based on criteria like city name, case field, etc.
    *   **Visualizations:** This panel shows various chart types available in Power BI, such as bar charts, line charts, pie charts, maps, and more. It also includes options for formatting visual elements (e.g., legend, x-axis, y-axis, tooltips).
    *   **Fields:** This panel lists available data fields (e.g., City(s), Country, UN 2018 population est(s)) that can be used in the visualization.
*   **Power BI Interface Elements:** The standard Power BI menu bar (File, Home, Insert, Modeling, View, Data/Drill Through) is visible at the top. There is also a search bar.
*   **Text:** Some other key text elements include:
    *   City(s)
    *   UN 2018 population estimates(s)
    *   Filters on this visual
    *   Filters on this page
    *   Fields
    *   Add data fields here
*   **Person:** In the lower right corner is a headshot of a person wearing a headset, likely a presenter or user interacting with the Power BI software.

**3. Purpose or educational value:**

The image is likely part of a tutorial or demonstration of Power BI's capabilities. It highlights the creation of a visualization (bar chart) to analyze population data by city. It also showcases how to use filters, visualization options, and data fields within Power BI to explore and present data insights. The intention seems to be educational, showing how users can create interactive dashboards and reports using Power BI.

**4. Specific technical details:**

*   The bar chart is sorted in descending order of population size, allowing for easy comparison of the most populous cities.
*   The Power BI workspace is customized, showing filters, visualizations, and field list panels to help analyze data effectively.

In summary, this image demonstrates the creation of a population data visualization in Power BI. It serves as an example of how to analyze data and customize interactive reports in Power BI.

*Original image: ![Smart narratives with Power BI](https://i.ytimg.com/vi_webp/eHmvCNhZiWg/sddefault.webp)*](https://youtu.be/eHmvCNhZiWg)

## Apache Airflow

1. [Overview of Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/)
2. [Airflow Playlist](https://www.youtube.com/playlist?list=PL5_c35Deekdm6N1OBHdQm7JZECTdm7zl-)
